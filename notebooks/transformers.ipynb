{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the Transformer Architecture\n",
    "\n",
    "most of the code here is a simplified version of [Karpathy's nanoGPT codebase](https://github.com/karpathy/nanoGPT/) -- check that out if you want to see a more practical implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import unicodedata\n",
    "import math\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import numpy as np\n",
    "from jaxtyping import Int, Float\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import transformers\n",
    "import transformer_lens\n",
    "\n",
    "from utils.get_books import get_gutenberg_book, get_many_books\n",
    "from utils.mermaid import mm\n",
    "# from utils.analyze_vocab import analyze_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic auto-reload for local development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to install required packages:\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overview\n",
    "\n",
    "- attention\n",
    "- homework check-in with GPT-4 demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So:\n",
    "- dense networks fail because they re-learn the same thing at every position\n",
    "- convolutional networks give the *spatial* prior to our networks, great for images\n",
    "- RNNs give the *temporal* prior to our networks, great for sequences -- but they decay over time\n",
    "\n",
    "When we actually process language, we care about more than just *local* relationships of this sort:\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/8/8c/Parse2.jpg)\n",
    "\n",
    "Many scientists tried for a very long time to try to encode parse trees in machines, but using human knowledge ended up being far less effective than letting the machines pick it up themselves.\n",
    "\n",
    "The key insight of transformers and modern large language models is the *attention mechanism:*\n",
    "\n",
    "> Given some sequence of inputs $[x_1, \\ldots, x_n]$, when trying to predict the next token $x_{n+1}$, long range dependencies can be captured by letting a network choose which previous tokens to pay *attention* to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> note on notation: we're now switching to $x_i$ being already embedded vectors in $\\R^{d_m}$, rather than one-hot vectors in $\\R^{d_V}$\n",
    "\n",
    "The attention mechanism is a map $\\mathbb{A} : \\R^{d_m} \\times (\\R^{d_m})^{n_c} \\to \\R^{d_m}$ which takes a *query* $x_q$ and a *key* $x_k$ and produces a weighted sum of the values. We first consider the computation of scalar attention $A_s: \\R^{d_m} \\times \\R^{d_m} \\to \\R$ for some head $s$.\n",
    "\n",
    "$$ A_s(x_q, x_k) = \\sigma \\left( c \\cdot x_q Q_s (x_k K_s)^T \\right) $$\n",
    "\n",
    "What are all these symbols? Firstly, we define $d_h$: the *head dimension*. We usually pick a number of heads `n_heads` and set $d_h = d_m / n_h$. Think of each head as a separate attention mechanism, kind of like different kernels in a convolutional layer.\n",
    "\n",
    "- $c$ is a constant scalar, usually set to $\\frac{1}{\\sqrt{d_h}}$\n",
    "- $Q_s, K_s \\in \\R^{d_m \\times d_h}$ are learned linear maps - we have a different set of weights for each head\n",
    "- $\\sigma$ is the softmax function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once we have the scalar attention $A_s$, we can define the full attention map $A: \\R^{d_m} \\times \\R^{d_m} \\to \\R^{d_m}$ as follows:\n",
    "\n",
    "$$ \\mathbb{A}(x_q) = [ A_s(x_q, x_k) x_q V_s ]_{s \\in \\N_{n_h}} $$\n",
    "\n",
    "$$ = \\left[ \\sigma \\left( \\frac{1}{\\sqrt{d_h}} \\cdot x_q Q_s (x_k K_s)^T \\right) x_q V_s \\right]_{s \\in \\N_{n_h}} $$\n",
    "\n",
    "where $W^Q_i, W^K_i, W^V_i \\in \\R^{d_m \\times d_h}$ are learned linear maps, and $V_i \\in \\R^{d_m}$ are learned vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up configuration\n",
    "\n",
    "- `d_model` $= d_m$\n",
    "- `d_vocab` $= d_v$\n",
    "- `n_context` $= n_c$\n",
    "- `n_layer` $= n_L$\n",
    "- `n_head` $= n_h$\n",
    "- `d_head` $= d_h = \\frac{d_m}{n_h}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class GPTConfig:\n",
    "    \"\"\"defaults are the gpt-2 model\"\"\"\n",
    "    d_model: int = 768\n",
    "    d_vocab: int = 50257\n",
    "    n_context: int = 1024\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "\n",
    "    @property\n",
    "    def d_head(self):\n",
    "        assert self.d_model % self.n_head == 0, f\"'{self.d_model = }' must be divisible by '{self.n_head = }': {self.d_model} % {self.n_head} == {self.d_model % self.n_head}\"\n",
    "        return self.d_model // self.n_head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I kind of lied to you earlier about the attention heads -- they don't actually operate over a pair of vectors, since the softmax gets applied over all items for that key.\n",
    "\n",
    "Additionally, we implement *causal masking* by adding a matrix of $-\\infty$.\n",
    "\n",
    "\n",
    "Given some input $x \\in \\R^{n_c \\times d_m}$, we can express our attention head $\\mathbb{A}(x): \\R^{n_c \\times d_m} \\to \\R^{n_c \\times d_h}$ as:\n",
    "\n",
    "\n",
    "$$\n",
    "\t\\mathbb{A}(x) := \\left( \\frac{\\texttt{softmax}(x W_Q (x W_K)^T)}{\\sqrt{d_h}} + M \\right) x W_V\n",
    "$$\n",
    "\n",
    "Where $M$ is a causal mask, and\n",
    "\n",
    "$$ \n",
    "\t[M]_{i,j} = \\begin{cases} \n",
    "\t\t0 & i \\geq j \\\\\n",
    "\t\t-\\infty & i < j \\\\\n",
    "\t\\end{cases}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # store dimensions\n",
    "        self.n_head: int = config.n_head\n",
    "        self.d_model: int = config.d_model\n",
    "        self.n_context: int = config.n_context\n",
    "\n",
    "        # concatenating the outputs of the heads should give us d_model, but this check is done in GPTConfig\n",
    "        self.d_head: int = config.d_head\n",
    "\n",
    "        # coefficient for scaling the dot product of the query and key in the attention calculation\n",
    "        self.sqrt_dim: float = 1.0 / math.sqrt(self.d_head)\n",
    "\n",
    "        # key, query, value projections\n",
    "        self.W_K: nn.Module = nn.Linear(self.d_model, self.d_head)\n",
    "        self.W_Q: nn.Module = nn.Linear(self.d_model, self.d_head)\n",
    "        self.W_V: nn.Module = nn.Linear(self.d_model, self.d_head)\n",
    "\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        # `register_buffer` means it's not a trainable parameter\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\", \n",
    "            torch.tril(\n",
    "                torch.ones(config.n_context, config.n_context)\n",
    "            )\n",
    "            .view(1, 1, config.n_context, config.n_context)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"batch n_ctx d_model\"]) -> Float[torch.Tensor, \"batch n_ctx d_head\"]:\n",
    "        assert x.ndim == 3, str(x.shape)\n",
    "        B, n_ctx, d_model = x.shape # batch size, sequence length, embedding dimensionality (d_model)\n",
    "        assert d_model == self.d_model, str(x.shape)\n",
    "        # assert n_ctx == self.n_context, str(x.shape)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q: Float[torch.Tensor, \"batch n_ctx d_head\"] = self.W_Q(x)\n",
    "        k: Float[torch.Tensor, \"batch n_ctx d_head\"] = self.W_K(x)\n",
    "        v: Float[torch.Tensor, \"batch n_ctx d_head\"] = self.W_V(x)\n",
    "\n",
    "        # self-attention\n",
    "        # (B, n_ctx, d_h) x (B, d_h, n_ctx) -> (B, n_ctx, n_ctx)\n",
    "        att = (q @ k.transpose(-2, -1)) * self.sqrt_dim\n",
    "        \n",
    "        # autoregressive (causal) masking\n",
    "        att = att.masked_fill(\n",
    "            self.causal_mask[:,:n_ctx,:n_ctx] == 0, \n",
    "            float('-inf'),\n",
    "        )\n",
    "\n",
    "        # softmax\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        # apply the self-attention to the values\n",
    "        # (B, n_ctx, n_ctx) x (B, n_ctx, d_h) -> (B, n_ctx, d_h)\n",
    "        output = att @ v\n",
    "        return output.view(B, n_ctx, self.d_head)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, you might notice that we also add another dimension `B` -- this is the *batch dimension*, and speeds things up a lot when we want to compute gradients over a set of samples. PyTorch broadcasts all operations over that batch dimension, so all the calculations are equivalent.\n",
    "\n",
    "Let's check to see if the dimensions are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionHead(\n",
      "  (W_K): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (W_Q): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (W_V): Linear(in_features=64, out_features=16, bias=True)\n",
      ")\n",
      "cfg.d_head = 16\n",
      "x.shape = torch.Size([1, 128, 64])\n",
      "A(x).shape = torch.Size([1, 128, 16])\n"
     ]
    }
   ],
   "source": [
    "cfg: GPTConfig = GPTConfig(\n",
    "\tn_context=128,\n",
    "\td_model=64,\n",
    "\tn_head=4,\n",
    ")\n",
    "A: AttentionHead = AttentionHead(cfg)\n",
    "print(A)\n",
    "\n",
    "x = torch.randn(1, cfg.n_context, cfg.d_model)\n",
    "print(f\"{cfg.d_head = }\")\n",
    "print(f\"{x.shape = }\")\n",
    "print(f\"{A(x).shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "does that look correct?\n",
    "\n",
    "\n",
    "Now, let's take a look at that causal mask $M$, referred to as `causal_mask` in the code. You'll note that when we define it, it's actually a matrix of $\\{0, 1\\}$ and not $\\{-\\infty, 0\\}$. However, in the `.forward()` function, we use `masked_fill` to set the elements of `attn` to $-\\infty$ where $M$ is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(A.causal_mask[0, 0].cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-headed attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of each attention head as learning some *concept* -- but we have many concepts we might need to learn, hence multiple attention heads. Let's call this function $\\text{MHA}(x) : \\R^{n_c \\times d_m} \\to \\R^{n_c \\times d_m}$. Given attention heads $[\\mathbb{A}_1, \\ldots, \\mathbb{A}_{n_h}]$, we compute `MHA` as:\n",
    "\n",
    "\n",
    "$$\n",
    "\t\\texttt{MHA}(x) = \\left[\\begin{array}{c} \n",
    "\t\t\\mathbb{A}_1(x) \\\\ \\hline\n",
    "\t\t\\mathbb{A}_2(x) \\\\ \\hline\n",
    "\t\t\\vdots \\\\ \\hline\n",
    "\t\t\\mathbb{A}_{n_h}(x)\n",
    "\t\\end{array}\\right]\n",
    "\t\\cdot [x W_O]\n",
    "$$\n",
    "\n",
    "Where $W_O \\in \\R^{d_m \\times d_m}$ is just another learned linear map.\n",
    "\n",
    "\n",
    "In our implementation, for readability and simplicity, we compute the attention heads one by one and then concatenate them. This is really really inefficient -- in reality you would do this all in one matrix multiplication, but the indexing for that gets complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "\tdef __init__(self, config: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.n_head: int = config.n_head\n",
    "\t\tself.d_head: int = config.d_model // config.n_head\n",
    "\t\tself.d_model: int = config.d_model\n",
    "\n",
    "\t\t# attention heads\n",
    "\t\tself.attention_heads: nn.ModuleList = nn.ModuleList([\n",
    "\t\t\tAttentionHead(config) \n",
    "\t\t\tfor _ in range(self.n_head)\n",
    "\t\t])\n",
    "\n",
    "\t\t# output projection\n",
    "\t\tself.W_O: nn.Module = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"batch n_ctx d_model\"]) -> Float[torch.Tensor, \"batch n_ctx d_model\"]:\n",
    "\t\tassert x.ndim == 3, str(x.shape)\n",
    "\t\t# apply all attention heads and concatenate their outputs\n",
    "\t\t# note: in reality, you would do this all in one tensor\n",
    "\t\t# we split the attention heads up to make it easier to understand\n",
    "\t\tatt = torch.cat(\n",
    "\t\t\t[\n",
    "\t\t\t\thead(x) \n",
    "\t\t\t\tfor head in self.attention_heads\n",
    "\t\t\t],\n",
    "\t\t\tdim=-1,\n",
    "\t\t)\n",
    "\t\tassert len(att.shape) == 3, str(att.shape)\n",
    "\n",
    "\t\t# output projection\n",
    "\t\t# (B, n_ctx, d_head * n_head) -> (B, n_ctx, d_model)\n",
    "\t\toutput = self.W_O(att)\n",
    "\t\tassert output.shape == x.shape, str(output.shape)\n",
    "\t\treturn output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1: a Transformer](assets/transformers_robot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformer is made up of *Transformer Blocks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFREOwpzdWJncmFwaCBUcmFuc2Zvcm1lckJsb2NrCiAgICB4KCh4KSkKICAgIHggLS0+IGxuXzEKCWxuXzEgLS0+IE1IQQoJTUhBIC0tPiBsbl8yCiAgICB4IC0tPiBsbl8yCglsbl8yIC0tPiBNTFAKCXkoKHkpKQogICAJTUxQIC0tPiB5CiAgICB4IC0tPiB5CmVuZAo=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "mm(\"\"\"\n",
    "graph TD;\n",
    "subgraph TransformerBlock\n",
    "    x((x))\n",
    "    x --> ln_1\n",
    "\tln_1 --> MHA\n",
    "\tMHA --> ln_2\n",
    "    x --> ln_2\n",
    "\tln_2 --> MLP\n",
    "\ty((y))\n",
    "   \tMLP --> y\n",
    "    x --> y\n",
    "end\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\tdef __init__(self, config: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t# layernorm, attention, another layernorm, mlp\n",
    "\t\tself.ln_1: nn.Module = nn.LayerNorm(config.d_model)\n",
    "\t\tself.attention: nn.Module = MultiHeadedAttention(config)\n",
    "\t\tself.ln_2: nn.Module = nn.LayerNorm(config.d_model)\n",
    "\t\tself.mlp: nn.Module = nn.Sequential(\n",
    "\t\t\tnn.Linear(config.d_model, 4 * config.d_model),\n",
    "\t\t\tnn.GELU(),\n",
    "\t\t\tnn.Linear(4 * config.d_model, config.d_model),\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"batch n_ctx d_model\"]) -> Float[torch.Tensor, \"batch n_ctx d_model\"]:\n",
    "\t\tassert x.ndim == 3, str(x.shape)\n",
    "\t\tx = x + self.attention(self.ln_1(x))\n",
    "\t\tassert x.ndim == 3, str(x.shape)\n",
    "\t\tx = x + self.mlp(self.ln_2(x))\n",
    "\t\tassert x.ndim == 3, str(x.shape)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's this `LayerNorm` thing?\n",
    "\n",
    "see: https://arxiv.org/abs/1607.06450\n",
    "\n",
    "$$ \\texttt{LayerNorm}(x) = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, for the GPT itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIFREOwpzdWJncmFwaCBHUFQKICAgIHByb21wdCgocHJvbXB0KSkKICAgIHd0ZVtcVG9rZW4gRW1iZWRkaW5nL10KCXdwZXt7UG9zaXRpb25hbCBFbmNvZGluZ319CiAgICBUQjFbW1RyYW5zZm9ybWVyQmxvY2tfMV1dCiAgICBUQjJbW1RyYW5zZm9ybWVyQmxvY2tfMl1dCglUQl9kb3Rzey4uLn0KICAgIFRCbkxbW1RyYW5zZm9ybWVyQmxvY2tfbl9MXV0KICAgIGxuX2ZbW0xheWVyTm9ybV1dCiAgICBsbV9oZWFkWy9EZS1FbWJlZGRpbmdcXQogICAKICAgIHByb21wdCAtLT4gd3RlCiAgICBwcm9tcHQgLS0+IHdwZQogICAgd3RlIC0tPiBUQjEKICAgIHdwZSAtLT4gVEIxCiAgICBUQjEgLS0+IFRCMgogICAgVEIyIC0tPiBUQl9kb3RzCiAgICBUQl9kb3RzIC0tPiBUQm5MCiAgICBUQm5MIC0tPiBsbl9mCiAgICBsbl9mIC0tPiBsbV9oZWFkCmVuZAo=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm(\"\"\"\n",
    "graph TD;\n",
    "subgraph GPT\n",
    "    prompt((prompt))\n",
    "    wte[\\Token Embedding/]\n",
    "\twpe{{Positional Encoding}}\n",
    "    TB1[[TransformerBlock_1]]\n",
    "    TB2[[TransformerBlock_2]]\n",
    "\tTB_dots{...}\n",
    "    TBnL[[TransformerBlock_n_L]]\n",
    "    ln_f[[LayerNorm]]\n",
    "    lm_head[/De-Embedding\\]\n",
    "   \n",
    "    prompt --> wte\n",
    "    prompt --> wpe\n",
    "    wte --> TB1\n",
    "    wpe --> TB1\n",
    "    TB1 --> TB2\n",
    "    TB2 --> TB_dots\n",
    "    TB_dots --> TBnL\n",
    "    TBnL --> ln_f\n",
    "    ln_f --> lm_head\n",
    "end\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\tdef __init__(self, config: GPTConfig, tokenizer: transformers.PreTrainedTokenizer):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.config: GPTConfig = config\n",
    "\t\tself.tokenizer: transformers.PreTrainedTokenizer = tokenizer\n",
    "\t\tassert config.d_vocab >= tokenizer.vocab_size\n",
    "\n",
    "\t\t# token and positional embeddings\n",
    "\t\tself.token_embeddings: nn.Module = nn.Embedding(config.d_vocab, config.d_model)\n",
    "\t\tself.positional_embeddings: nn.Module = nn.Embedding(config.n_context, config.d_model)\n",
    "\n",
    "\t\t# transformer\n",
    "\t\tself.transformer_blocks: nn.ModuleList = nn.ModuleList([\n",
    "\t\t\tTransformerBlock(config) \n",
    "\t\t\tfor _ in range(config.n_layer)\n",
    "\t\t])\n",
    "\n",
    "\t\t# language model head\n",
    "\t\tself.ln_f: nn.Module = nn.LayerNorm(config.d_model)\n",
    "\t\tself.lm_head: nn.Module = nn.Linear(config.d_model, config.d_vocab, bias=False)\n",
    "\n",
    "\tdef forward(\n",
    "\t\t\tself, \n",
    "\t\t\tx: Int[torch.Tensor, \"batch n_ctx\"],\n",
    "\t\t\ttargets: Int[torch.Tensor, \"batch n_ctx\"]|None = None,\n",
    "\t\t) -> tuple:\n",
    "\t\t\"\"\"returns a tuple of (logits, loss) where loss=None if targets is None\"\"\"\n",
    "\t\tassert x.ndim == 2, str(x.shape)\n",
    "\n",
    "\t\t# calculate token and positional embeddings and sum them\n",
    "\t\tx_res = self.token_embeddings(x) + self.positional_embeddings(torch.arange(x.size(1), device=x.device))\n",
    "\n",
    "\t\tassert x_res.ndim == 3, str(x.shape)\n",
    "\n",
    "\t\t# transformer blocks\n",
    "\t\tfor i, block in enumerate(self.transformer_blocks):\n",
    "\t\t\tx_res = block(x_res)\n",
    "\n",
    "\t\t# language model head\n",
    "\t\tlogits: Float[torch.Tensor, \"batch n_ctx d_vocab\"] = self.lm_head(self.ln_f(x_res))\n",
    "\n",
    "\t\tloss = None\n",
    "\t\tif targets is not None:\n",
    "\t\t\tloss = F.cross_entropy(\n",
    "\t\t\t\tlogits.transpose(1, 2),\n",
    "\t\t\t\ttargets,\n",
    "\t\t\t\tignore_index=-1,\n",
    "\t\t\t)\n",
    "\n",
    "\t\treturn logits, loss\n",
    "\t\n",
    "\t@torch.no_grad()\n",
    "\tdef generate(\n",
    "\t\tself,\n",
    "\t\tprompt: str|list[int]|Int[torch.Tensor, \"* n_ctx\"],\n",
    "\t\tmax_new_tokens: int = 128,\n",
    "\t\ttemperature: float = 1.0,\n",
    "\t) -> str:\n",
    "\n",
    "\t\t# convert prompt to string and tensor versions\n",
    "\t\tprompt_str: str\n",
    "\t\tprompt_tensor: Int[torch.Tensor, \"1 n_ctx\"]\n",
    "\t\tif isinstance(prompt, str):\n",
    "\t\t\tprompt_str = prompt\n",
    "\t\t\tprompt_tensor = torch.tensor(self.tokenizer.encode(prompt_str), dtype=torch.long).unsqueeze(0) # add batch dim\n",
    "\t\telif isinstance(prompt, list):\n",
    "\t\t\tprompt_str = self.tokenizer.decode(prompt)\n",
    "\t\t\tprompt_tensor = torch.tensor(prompt, dtype=torch.long).unsqueeze(0) # add batch dim\n",
    "\t\telif isinstance(prompt, torch.Tensor):\n",
    "\t\t\tif prompt.ndim == 1:\n",
    "\t\t\t\tprompt = prompt.unsqueeze(0) # add batch dim\n",
    "\t\t\tassert prompt.ndim == 2\n",
    "\n",
    "\t\t\tprompt_str = self.tokenizer.decode(prompt[0].tolist())\n",
    "\t\t\tprompt_tensor = prompt\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(f\"prompt must be a string, list of ints, or PyTorch tensor\")\n",
    "\t\t\n",
    "\t\t# check tensor dims\n",
    "\t\tassert isinstance(prompt_str, str) \n",
    "\t\tassert isinstance(prompt_tensor, torch.Tensor)\n",
    "\t\tassert prompt_tensor.ndim == 2 \n",
    "\t\tassert prompt_tensor.shape[0] == 1\n",
    "\n",
    "\t\t#  device\n",
    "\t\tprompt_tensor = prompt_tensor.to(self.device)\n",
    "\n",
    "\t\t# pad the prompt if necessary\n",
    "\t\tif prompt_tensor.shape[1] < self.config.n_context:\n",
    "\t\t\tprompt_tensor = F.pad(prompt_tensor, (0, self.config.n_context - prompt_tensor.shape[1]), value=self.tokenizer.pad_token_id)\n",
    "\n",
    "\t\tassert prompt_tensor.shape[1] == self.config.n_context\n",
    "\n",
    "\t\t# iterate until max_new_tokens is reached, or an end-of-sequence token is generated\n",
    "\t\tcompletions: list[int] = list()\n",
    "\t\tfor _ in range(max_new_tokens):\n",
    "\t\t\t# truncate sequence to block size\n",
    "\t\t\tprompt_len: int = prompt_tensor.shape[1]\n",
    "\t\t\tif prompt_len > self.config.n_context:\n",
    "\t\t\t\tprompt_tensor = prompt_tensor[:, -self.config.n_context:]\n",
    "\n",
    "\t\t\t# forward the model to get the logits for the index in the sequence\n",
    "\t\t\tlogits, _ = self(prompt_tensor)\n",
    "\n",
    "\t\t\t# pluck the logits at the final step and scale by desired temperature\n",
    "\t\t\tlogits = logits[:, -1, :] / temperature\n",
    "\n",
    "\t\t\t# apply softmax to convert logits to (normalized) probabilities\n",
    "\t\t\tprobs = F.softmax(logits, dim=-1)\n",
    "\n",
    "\t\t\t# sample from the distribution\n",
    "\t\t\tidx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\t\t\t# append sampled index to the running sequence and continue\n",
    "\t\t\tidx = torch.cat((prompt_tensor, idx_next), dim=1)\n",
    "\n",
    "\t\t\t# append the token to the running completions\n",
    "\t\t\tcompletions.append(int(idx_next[0, 0]))\n",
    "\n",
    "\t\t\t# check if end of sequence token is generated\n",
    "\t\t\tif idx_next == self.tokenizer.eos_token_id:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\treturn self.tokenizer.decode(completions)\n",
    "\n",
    "\t@property\n",
    "\tdef n_params(self) -> int:\n",
    "\t\treturn sum(p.numel() for p in self.parameters())\n",
    "\t\n",
    "\t@property\n",
    "\tdef device(self) -> torch.device:\n",
    "\t\tdevice_set: set[torch.device] = set(p.device for p in self.parameters())\n",
    "\t\tassert len(device_set) == 1, device_set\n",
    "\t\treturn next(iter(device_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting book 84...\n",
      "\t426814 characters read\n",
      "Getting book 15...\n",
      "\t1241059 characters read\n",
      "Getting book 18...\n",
      "\t1191618 characters read\n",
      "Getting book 82...\n",
      "\t1125006 characters read\n",
      "Getting book 996...\n",
      "\t2342282 characters read\n",
      "Getting book 2600...\n",
      "\t3274018 characters read\n"
     ]
    }
   ],
   "source": [
    "text_data: str = ' '.join(get_many_books([84, 15, 18, 82, 996, 2600]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\tdef __init__(\n",
    "\t\t\tself, \n",
    "\t\t\ttext: str, \n",
    "\t\t\ttokenizer: transformers.PreTrainedTokenizer,\n",
    "\t\t\tn_context: int,\n",
    "\t\t\tensure_n_context_match: bool = True,\n",
    "\t\t):\n",
    "\t\t# add 1 to n_context to account for the target token\n",
    "\t\tn_context += 1\n",
    "\n",
    "\t\t# tokenize the text\n",
    "\t\ttokenized_text: list[int] = tokenizer.encode(text)\n",
    "\t\tself.total_tokens: int = len(tokenized_text)\n",
    "\n",
    "\t\t# trim the last tokens to make sure the length is a multiple of n_context\n",
    "\t\tif ensure_n_context_match:\n",
    "\t\t\ttokenized_text = tokenized_text[:-(len(tokenized_text) % n_context)]\n",
    "\t\t\tself.total_tokens = len(tokenized_text)\n",
    "\n",
    "\t\t# split the text into examples of length n_context\n",
    "\t\t# this means that text will often start in the middle of a sentence\n",
    "\t\t# in reality, we might want to do this a bit smarter\n",
    "\t\tself.examples: list[list[int]] = [\n",
    "\t\t\ttokenized_text[i:i+n_context] \n",
    "\t\t\tfor i in range(0, len(tokenized_text), n_context)\n",
    "\t\t]\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.examples)\n",
    "\t\n",
    "\tdef __getitem__(self, i: int) -> Float[torch.Tensor, \"n_ctx\"]:\n",
    "\t\treturn torch.tensor(self.examples[i], dtype=torch.long)\n",
    "\t\n",
    "\tdef example_lengths(self) -> Counter[int]:\n",
    "\t\treturn Counter(len(ex) for ex in self.examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "\tmodel: GPT,\n",
    "\ttext: str,\n",
    "\toptimizer: torch.optim.Optimizer,\n",
    "\tdevice: torch.device = (\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\tbatch_size: int = 8,\n",
    "\tmax_batches: int|None = None,\n",
    "\tprint_interval: int = 100,\n",
    "\tepochs: int = 1,\n",
    ") -> tuple[GPT, list[dict]]:\n",
    "\t\n",
    "\t# move model to device\n",
    "\tprint(f\"moving model to device: {device}\")\n",
    "\tmodel.to(device)\n",
    "\t\n",
    "\t# set up data\n",
    "\tprint(f\"setting up dataset from text of length {len(text)}\")\n",
    "\tdataset: TextDataset = TextDataset(\n",
    "\t\ttext=text, \n",
    "\t\ttokenizer=model.tokenizer, \n",
    "\t\tn_context=model.config.n_context,\n",
    "\t)\n",
    "\tprint(f\"\\tset up dataset with {len(dataset)} examples, example lengths: {dataset.example_lengths()}\")\n",
    "\n",
    "\tprint(f\"setting up dataloader from {len(dataset)} examples\")\n",
    "\tdataloader: DataLoader = DataLoader(\n",
    "\t\tdataset, \n",
    "\t\tbatch_size=batch_size, \n",
    "\t\tshuffle=True,\n",
    "\t)\n",
    "\tprint(f\"\\tset up dataloader with {len(dataloader)} batches of size {batch_size}\")\n",
    "\n",
    "\t# set up training loop\n",
    "\tprint(\"training...\")\n",
    "\ttraining_records: list[dict] = list()\n",
    "\tmodel.train()\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tprint(f\"Epoch {epoch + 1}/{epochs}\\n\")\n",
    "\t\ti: int; batch: Float[torch.Tensor, \"batch n_ctx\"]\n",
    "\t\tfor i, batch in tqdm.tqdm(\n",
    "\t\t\tenumerate(dataloader),\n",
    "\t\t\ttotal=len(dataloader),\n",
    "\t\t\tdesc=\"Training\",\n",
    "\t\t):\n",
    "\t\t\t# move batch to device\n",
    "\t\t\tbatch = batch.to(device)\n",
    "\t\t\t\n",
    "\t\t\t# break if we've reached the maximum number of batches\n",
    "\t\t\tif max_batches is not None and i > max_batches:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\t# forward pass\n",
    "\t\t\tlogits, loss = model(\n",
    "\t\t\t\tbatch[:, :-1],\n",
    "\t\t\t\ttargets=batch[:, 1:], # the targets are just the input, offset by one\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# backward pass\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# record progress\n",
    "\t\t\ttraining_records.append({\n",
    "\t\t\t\t\"batch\": i,\n",
    "\t\t\t\t\"loss\": loss.item(),\n",
    "\t\t\t})\n",
    "\n",
    "\t\t\tif i % print_interval == 0:\n",
    "\t\t\t\tprint(f\"Batch {i}, Loss: {loss.item()}\\n\")\n",
    "\n",
    "\treturn model, training_records\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENIZER.vocab_size = 50257\n",
      "MODEL.n_params = 32115712\n"
     ]
    }
   ],
   "source": [
    "# we want to ensure our vocab dimension is the same as the tokenizer's vocab size\n",
    "TOKENIZER: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(f\"{TOKENIZER.vocab_size = }\")\n",
    "\n",
    "# set up a config for a small model\n",
    "CONFIG: GPTConfig = GPTConfig(\n",
    "\td_model=256,\n",
    "\td_vocab=TOKENIZER.vocab_size,\n",
    "\tn_context=256,\n",
    "\tn_layer=8,\n",
    "\tn_head=8,\n",
    ")\n",
    "\n",
    "# initialize the model\n",
    "MODEL: GPT = GPT(CONFIG, TOKENIZER)\n",
    "print(f\"{MODEL.n_params = }\")\n",
    "\n",
    "# optimizer\n",
    "OPTIMIZER: torch.optim.Optimizer = torch.optim.AdamW(MODEL.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving model to device: cuda\n",
      "setting up dataset from text of length 9600802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2603696 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tset up dataset with 10131 examples, example lengths: Counter({257: 10131})\n",
      "setting up dataloader from 10131 examples\n",
      "\tset up dataloader with 317 batches of size 32\n",
      "training...\n",
      "Epoch 1/5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/317 [00:10<53:00, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 11.00926685333252\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|      | 101/317 [13:20<28:28,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 6.921087741851807\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|   | 201/317 [26:30<15:17,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 6.195537090301514\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|| 301/317 [39:41<02:06,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 5.990717887878418\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 317/317 [41:47<00:00,  7.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/317 [00:08<43:08,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.90175199508667\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|      | 101/317 [13:18<28:25,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 5.742660999298096\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|   | 201/317 [26:29<15:17,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200, Loss: 5.630425453186035\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|| 301/317 [39:48<02:07,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300, Loss: 5.613412380218506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 317/317 [41:54<00:00,  7.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/317 [00:08<43:32,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 5.435191631317139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|         | 19/317 [02:30<39:15,  7.90s/it]"
     ]
    }
   ],
   "source": [
    "MODEL_TRAINED, training_history = train(\n",
    "\tmodel=MODEL,\n",
    "\ttext=text_data,\n",
    "\toptimizer=OPTIMIZER,\n",
    "\tdevice=(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\tbatch_size=32,\n",
    "\tmax_batches=None,\n",
    "\tprint_interval=100,\n",
    "\tepochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(MODEL_TRAINED, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MODEL_TRAINED.generate(\"The \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
